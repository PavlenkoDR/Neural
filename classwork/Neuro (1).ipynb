{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Neuro.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ws0gfCeikzb",
        "colab_type": "text"
      },
      "source": [
        "Распознавание цветов (ромашки, розы, подсолнухи, тюльпаны, одуванчики) на изображениях с помощью предварительно обученной нейронной сети VGG16"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcZCbKdRitNi",
        "colab_type": "text"
      },
      "source": [
        "Для распознавания используется предварительно обученная сверточная нейронная сеть VGG16.\n",
        "\n",
        "Перед использованием необходимо скачать и подготовить данные для обучения, проверки и тестирования."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_dDvKpNHXmAE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.python.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.python.keras.models import Sequential\n",
        "from tensorflow.python.keras.layers import Activation, Dropout, Flatten, Dense\n",
        "from tensorflow.python.keras.applications import VGG16\n",
        "from tensorflow.python.keras.optimizers import Adam\n",
        "\n",
        "from tensorflow.python.keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
        "from tensorflow.python.keras.layers.normalization import BatchNormalization\n",
        "\n",
        "from tensorflow.python.keras.applications import InceptionV3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Q6bZRqlXsTv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Каталог с данными для обучения\n",
        "train_dir = 'train'\n",
        "# Каталог с данными для проверки\n",
        "val_dir = 'val'\n",
        "# Каталог с данными для тестирования\n",
        "test_dir = 'test'\n",
        "# Размеры изображения\n",
        "img_width, img_height = 150, 150\n",
        "# Размерность тензора на основе изображения для входных данных в нейронную сеть\n",
        "# backend Tensorflow, channels_last\n",
        "input_shape = (img_width, img_height, 3)\n",
        "# Размер мини-выборки\n",
        "batch_size = 32\n",
        "# Количество изображений для обучения\n",
        "nb_train_samples = 2565\n",
        "# Количество изображений для проверки\n",
        "nb_validation_samples = 550\n",
        "# Количество изображений для тестирования\n",
        "nb_test_samples = 550"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QH-YO48AYYy3",
        "colab_type": "text"
      },
      "source": [
        "Загружаем предварительно обученную нейронную сеть"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vpsEGunoYZfT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vgg16_net = VGG16(weights='imagenet', include_top=False, input_shape=(img_width, img_height, 3))\n",
        "# Загружаем предварительно обученную нейронную сеть Inceptionv3\n",
        "#inception_net = InceptionV3(weights='imagenet', include_top=False, input_shape=(150, 150, 3))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJAXWZxuYjgo",
        "colab_type": "text"
      },
      "source": [
        "Создаем составную нейронную сеть на основе VGG16 или inception"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVQE49HlYoJP",
        "colab_type": "code",
        "outputId": "bd0866dc-4996-4363-e5e4-cb1dfe205e16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 782
        }
      },
      "source": [
        "vgg16_net.summary()\n",
        "#inception_net.summary()"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_10 (InputLayer)        (None, 150, 150, 3)       0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 150, 150, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 150, 150, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 75, 75, 64)        0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 75, 75, 128)       73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 75, 75, 128)       147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 37, 37, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 37, 37, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 37, 37, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 37, 37, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 18, 18, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 18, 18, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 9, 9, 512)         0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n",
            "=================================================================\n",
            "Total params: 14,714,688\n",
            "Trainable params: 14,714,688\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "giGnkcjAYmUv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "# Добавляем в модель сеть VGG16 вместо слоя\n",
        "model.add(vgg16_net)\n",
        "#model.add(inception_net)\n",
        "model.add(Flatten())\n",
        "model.add(Dense(256))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(5, activation='sigmoid'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjL-pLOiYqA5",
        "colab_type": "text"
      },
      "source": [
        "Компилируем составную нейронную сеть"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZjU_DPCYsK5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(lr=1e-5), \n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ckqFeOrYwsn",
        "colab_type": "text"
      },
      "source": [
        "Создаем генератор изображений\n",
        "Генератор изображений создается на основе класса ImageDataGenerator. Генератор делит значения всех пикселов изображения на 255."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9MpXsAQzYwew",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "datagen = ImageDataGenerator(rescale=1./255)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7SB9STfY3B-",
        "colab_type": "text"
      },
      "source": [
        "Генератор данных для обучения на основе изображений из каталога"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnis05j4Y3aJ",
        "colab_type": "code",
        "outputId": "d105aad3-f0f5-4e77-ba94-a80dc75806a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_generator = datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical')"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 2565 images belonging to 5 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7JPjVwNY7TA",
        "colab_type": "text"
      },
      "source": [
        "Генератор данных для проверки на основе изображений из каталога"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6mmXl6BZAwT",
        "colab_type": "code",
        "outputId": "5122c000-e271-4237-e985-19786ac4b268",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "val_generator = datagen.flow_from_directory(\n",
        "    val_dir,\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical')"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 550 images belonging to 5 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0kcrT_UY-Wz",
        "colab_type": "text"
      },
      "source": [
        "Генератор данных для тестирования на основе изображений из каталога"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-8w2yDtZDi3",
        "colab_type": "code",
        "outputId": "5ac8ae46-b435-4a0c-88b2-cbedf5ae5798",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test_generator = datagen.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical')"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 550 images belonging to 5 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EgP7Y7YRZHXC",
        "colab_type": "text"
      },
      "source": [
        "Обучаем модель с использованием генераторов\n",
        "train_generator - генератор данных для обучения\n",
        "\n",
        "validation_data - генератор данных для проверки"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0eDIpbqaZJjL",
        "colab_type": "code",
        "outputId": "14388cdb-1b84-4545-975e-fc2962e10130",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        }
      },
      "source": [
        "model.fit_generator(\n",
        "    train_generator,\n",
        "    steps_per_epoch=nb_train_samples // batch_size,\n",
        "    epochs=10,\n",
        "    validation_data=val_generator,\n",
        "    validation_steps=nb_validation_samples // batch_size)"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "18/18 [==============================] - 4s 208ms/step - loss: 1.6147 - acc: 0.3055\n",
            "81/81 [==============================] - 24s 300ms/step - loss: 4.4236 - acc: 0.2246 - val_loss: 1.6147 - val_acc: 0.3055\n",
            "Epoch 2/10\n",
            "18/18 [==============================] - 2s 121ms/step - loss: 1.5261 - acc: 0.3691\n",
            "81/81 [==============================] - 21s 264ms/step - loss: 1.7328 - acc: 0.2655 - val_loss: 1.5261 - val_acc: 0.3691\n",
            "Epoch 3/10\n",
            "18/18 [==============================] - 2s 125ms/step - loss: 1.4719 - acc: 0.5073\n",
            "81/81 [==============================] - 22s 267ms/step - loss: 1.5972 - acc: 0.3142 - val_loss: 1.4719 - val_acc: 0.5073\n",
            "Epoch 4/10\n",
            "18/18 [==============================] - 2s 123ms/step - loss: 1.3964 - acc: 0.5309\n",
            "81/81 [==============================] - 22s 269ms/step - loss: 1.5258 - acc: 0.3676 - val_loss: 1.3964 - val_acc: 0.5309\n",
            "Epoch 5/10\n",
            "18/18 [==============================] - 2s 123ms/step - loss: 1.2841 - acc: 0.5855\n",
            "81/81 [==============================] - 22s 271ms/step - loss: 1.4593 - acc: 0.4316 - val_loss: 1.2841 - val_acc: 0.5855\n",
            "Epoch 6/10\n",
            "18/18 [==============================] - 2s 120ms/step - loss: 1.2309 - acc: 0.6364\n",
            "81/81 [==============================] - 22s 273ms/step - loss: 1.4209 - acc: 0.5232 - val_loss: 1.2309 - val_acc: 0.6364\n",
            "Epoch 7/10\n",
            "18/18 [==============================] - 2s 122ms/step - loss: 1.1467 - acc: 0.6182\n",
            "81/81 [==============================] - 22s 274ms/step - loss: 1.4138 - acc: 0.5540 - val_loss: 1.1467 - val_acc: 0.6182\n",
            "Epoch 8/10\n",
            "18/18 [==============================] - 2s 126ms/step - loss: 1.0445 - acc: 0.6964\n",
            "81/81 [==============================] - 22s 276ms/step - loss: 1.1541 - acc: 0.6511 - val_loss: 1.0445 - val_acc: 0.6964\n",
            "Epoch 9/10\n",
            "18/18 [==============================] - 2s 124ms/step - loss: 1.3788 - acc: 0.7255\n",
            "81/81 [==============================] - 22s 276ms/step - loss: 1.1375 - acc: 0.7197 - val_loss: 1.3788 - val_acc: 0.7255\n",
            "Epoch 10/10\n",
            "18/18 [==============================] - 2s 123ms/step - loss: 0.9018 - acc: 0.6855\n",
            "81/81 [==============================] - 23s 278ms/step - loss: 1.5339 - acc: 0.6483 - val_loss: 0.9018 - val_acc: 0.6855\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f66b77825f8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNuzbEPBZOjk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scores = model.evaluate_generator(test_generator, nb_test_samples // batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5Skuv9QZQji",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e9a16593-582b-4a4f-d675-e4eb0d867412"
      },
      "source": [
        "print(\"Аккуратность на тестовых данных: %.2f%%\" % (scores[1]*100))"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Аккуратность на тестовых данных: 63.45%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}